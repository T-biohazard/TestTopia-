{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIXhIsZRhRqvGxgDQVr/ei"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"N6mcg8EkPlVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain_community langchain_text_splitters langchain_openai langchain_chroma gradio python-dotenv pypdf"],"metadata":{"id":"o6Pz8wXqROUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","\n","# Load your .env file manually from your Drive\n","env_path = '/content/drive/MyDrive/MagicAI/.env'\n","load_dotenv(dotenv_path=env_path)"],"metadata":{"id":"H0zLkLJ0QwUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZswE-LrPPP4P"},"outputs":[],"source":["from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_chroma import Chroma\n","import gradio as gr\n","\n","# import the .env file\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","# configuration\n","'''DATA_PATH = r\"data\"\n","CHROMA_PATH = r\"chroma_db\"'''\n","\n","DATA_PATH = \"/content/drive/MyDrive/MagicAI/data\"\n","CHROMA_PATH = \"/content/drive/MyDrive/MagicAI/chroma_db\"\n","\n","\n","embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n","\n","# initiate the model\n","llm = ChatOpenAI(temperature=0.5, model='gpt-4o-mini')\n","\n","# connect to the chromadb\n","vector_store = Chroma(\n","    collection_name=\"example_collection\",\n","    embedding_function=embeddings_model,\n","    persist_directory=CHROMA_PATH,\n",")\n","\n","# Set up the vectorstore to be the retriever\n","num_results = 5\n","retriever = vector_store.as_retriever(search_kwargs={'k': num_results})\n","\n","# call this function for every message added to the chatbot\n","def stream_response(message, history):\n","    #print(f\"Input: {message}. History: {history}\\n\")\n","\n","    # retrieve the relevant chunks based on the question asked\n","    docs = retriever.invoke(message)\n","\n","    # add all the chunks to 'knowledge'\n","    knowledge = \"\"\n","\n","    for doc in docs:\n","        knowledge += doc.page_content+\"\\n\\n\"\n","\n","\n","    # make the call to the LLM (including prompt)\n","    if message is not None:\n","\n","        partial_message = \"\"\n","\n","        rag_prompt = f\"\"\"\n","        You are an assistent which answers questions based on knowledge which is provided to you.\n","        While answering, you don't use your internal knowledge,\n","        but solely the information in the \"The knowledge\" section.\n","        You don't mention anything to the user about the povided knowledge.\n","\n","        The question: {message}\n","\n","        Conversation history: {history}\n","\n","        The knowledge: {knowledge}\n","\n","        \"\"\"\n","\n","        #print(rag_prompt)\n","\n","        # stream the response to the Gradio App\n","        for response in llm.stream(rag_prompt):\n","            partial_message += response.content\n","            yield partial_message\n","\n","# initiate the Gradio app\n","chatbot = gr.ChatInterface(stream_response, textbox=gr.Textbox(placeholder=\"Send to the LLM...\",\n","    container=False,\n","    autoscroll=True,\n","    scale=7),\n",")\n","\n","# launch the Gradio app\n","chatbot.launch()"]}]}